{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk \n",
    "import string \n",
    "nltk.download('punkt')\n",
    "from statistics import median\n",
    "from statistics import mean\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from transformers import pipeline\n",
    "from spacy.pipeline import Sentencizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# need to downgrade numpy to before 2.0 \n",
    "# on windows, need to enable long paths : https://www.microfocus.com/documentation/filr/filr-4/filr-desktop/t47bx2ogpfz7.html \n",
    "# also need to do through REGEDIT on windows\n",
    "## if using windows 10, add gpedit.msc this way: https://www.reddit.com/r/AnnoyingTech/comments/ojru3t/adding_gpeditmsc_on_your_windows_home/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data (if downloading from github, concat the two parts)\n",
    "#df = pd.read_csv(\"11_6_fulldataset.csv\", index_col= 0)\n",
    "df1 = pd.read_csv(\"11_6_fulldatapart1.csv\")\n",
    "df2 = pd.read_csv(\"11_6_fulldatapart2.csv\")\n",
    "df= pd.concat([df1, df2], ignore_index = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning, processing, tagging\n",
    "## categorizing quoted by \n",
    "def process_quotes(s):\n",
    "    if \"Quoted By\" in s: \n",
    "        return re.findall(r'>>(\\d+)\\n', s)\n",
    "    else:\n",
    "        modified_string = s  # no modification needed if \"Quoted By\" is not present\n",
    "        return \"No Quote\"\n",
    "df['quotedby'] = df['Identifier'].apply(process_quotes)\n",
    "## removing it from the text \n",
    "def stripper (s): \n",
    "    if 'Quoted By' in s:\n",
    "        cleaned_string = re.sub(r'Quoted By:|>>\\d+\\n', '', s)\n",
    "        return cleaned_string.strip()\n",
    "    else: \n",
    "        return s\n",
    "df ['Text'] = df['Text'].apply(stripper)\n",
    "## getting the reply-to out \n",
    "df['replyto'] = df['Text'].apply(lambda text: re.findall(r'>>(\\d+)', text))\n",
    "df['Text'] = df['Text'].apply(lambda text: re.sub(r'>>\\d+\\s*', '', text).strip())\n",
    "# strip website links from the text\n",
    "# it means 'image of god' in latin \n",
    "sitepattern = r'(?:https?://|www\\.)\\S+|[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}(?:/[^\\s]*)?'\n",
    "df['Text'] = df['Text'].apply(lambda text: re.sub(sitepattern, '', text).strip())\n",
    "# strip 'imago dei' comments from the text\n",
    "df = df[~df['Text'].str.contains('imago', case=False, na=False)]\n",
    "df = df[~df['Text'].str.contains('amplissimus', case=False, na=False)]\n",
    "# strip Post Reply\n",
    "postpattern = r'Post\\nReply'\n",
    "df['Text'] = df['Text'].apply(lambda text: re.sub(postpattern, '', text).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to get rid of this pattern for the millionth time \n",
    "metapattern = r'.{5}(sameocrgoogleiqdbsaucenaotrace).*'\n",
    "df['Text'] = df['Text'].apply(lambda text: re.sub(metapattern, '', text ).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATIN EXTERMINATION!!! \n",
    "# lingua-py (https://github.com/pemistahl/lingua-py)\n",
    "languages = [Language.LATIN, Language.ENGLISH]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "def latin_exterminator(s):\n",
    "    confidence_value = detector.compute_language_confidence(s, Language.LATIN)\n",
    "    cv = float(f\"{confidence_value:.2f}\") \n",
    "    if cv >= 0.5:\n",
    "        return None\n",
    "    else: \n",
    "        return s\n",
    "\n",
    "#use the latin exterminator\n",
    "df['Text'] = df['Text'].apply(latin_exterminator)\n",
    "df = df[df['Text'].notnull()]\n",
    "\n",
    "# drop duplicates by anon-id (this only refers to the post, not the account)\n",
    "df = df.drop_duplicates(subset = 'anonid', keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PAUSE: we're doing sentence boundary testing now using pretty sophisticated methods, which take a while. let's use a smaller dataset first to see if it works. \n",
    "df = df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sentence detection => still work in progress \n",
    "\n",
    "# Load a spaCy language model (for Sentencizer, a lightweight \"blank\" model is enough)\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")  # Simply use the factory name as a string\n",
    "\n",
    "# Customize the sentencizer to include newline characters as sentence boundaries\n",
    "nlp.get_pipe(\"sentencizer\").punct_chars = [\".\", \"!\", \"?\", \"\\n\"]\n",
    "\n",
    "# Define the function using spaCy's sentencizer\n",
    "def spacy_sentsplit(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]  # filter out whitespace-only sentences\n",
    "    return sentences\n",
    "\n",
    "# Apply the function to get sentences\n",
    "df['Spacy_Sentences'] = df['Text'].apply(spacy_sentsplit)\n",
    "\n",
    "# Get sentence lengths\n",
    "def spacy_sentsplit_lengths(text):\n",
    "    sentences = spacy_sentsplit(text)\n",
    "    return [len(sentence) for sentence in sentences]\n",
    "\n",
    "df['Spacy_Sentence_Lengths'] = df['Text'].apply(spacy_sentsplit_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilyzou/opt/miniconda3/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "### spacy method: \n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = [\"ner\", \"tagger\"])\n",
    "def detect_sentences_spacy_pipe(text): \n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Function to get sentence lengths using spaCy method\n",
    "def spacy_sentence_lengths(text):\n",
    "    sentences = detect_sentences_spacy_pipe(text)\n",
    "    return [len(sentence) for sentence in sentences]\n",
    "\n",
    "df['Spacy_Sentences'] = df['Text'].apply(detect_sentences_spacy_pipe)\n",
    "df['Spacy_Sentence_Lengths'] = df['Text'].apply(spacy_sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence detection with NLTK's Punkt\n",
    "def nltk_sentsplit(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [sentence.strip() for sentence in sentences]\n",
    "\n",
    "df['NLTK_Sentences'] = df['Text'].apply(nltk_sentsplit)\n",
    "\n",
    "# Get sentence lengths with NLTK's Punkt\n",
    "def nltk_sentsplit_lengths(text):\n",
    "    sentences = nltk_sentsplit(text)\n",
    "    return [len(sentence) for sentence in sentences]\n",
    "\n",
    "df['NLTK_Sentence_Lengths'] = df['Text'].apply(nltk_sentsplit_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sentence segmentation pipeline using a Hugging Face model\n",
    "sentence_segmenter = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "# Sentence detection using Hugging Face Transformers\n",
    "def transformer_sentsplit(text):\n",
    "    segments = sentence_segmenter(text)\n",
    "    # Extract sentences from transformer pipeline output\n",
    "    return [segment['sentence'].strip() for segment in segments]\n",
    "\n",
    "df['Transformer_Sentences'] = df['Text'].apply(transformer_sentsplit)\n",
    "\n",
    "# Get sentence lengths with Hugging Face Transformers\n",
    "def transformer_sentsplit_lengths(text):\n",
    "    sentences = transformer_sentsplit(text)\n",
    "    return [len(sentence) for sentence in sentences]\n",
    "\n",
    "df['Transformer_Sentence_Lengths'] = df['Text'].apply(transformer_sentsplit_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def you_a_mismatch(row):\n",
    "    # Initialize a list to store sentence length tuples across methods\n",
    "    sentence_lengths = []\n",
    "    \n",
    "    # Use zip to pair sentences from all four methods by length\n",
    "    for regex, spacy, nltk, transformer in zip(\n",
    "        row['Regex_Sentences'], \n",
    "        row['Spacy_Sentences'], \n",
    "        row['NLTK_Sentences'], \n",
    "        row['Transformer_Sentences']\n",
    "    ):\n",
    "        sentence_lengths.append((len(regex), len(spacy), len(nltk), len(transformer)))\n",
    "    \n",
    "    # Add any extra sentences from Regex method if longer than others\n",
    "    sentence_lengths += [\n",
    "        (len(regex), 0, 0, 0) \n",
    "        for regex in row['Regex_Sentences'][len(row['Spacy_Sentences']):]\n",
    "    ]\n",
    "    \n",
    "    # Add extra sentences from SpaCy method if longer than others\n",
    "    sentence_lengths += [\n",
    "        (0, len(spacy), 0, 0) \n",
    "        for spacy in row['Spacy_Sentences'][len(row['Regex_Sentences']):]\n",
    "    ]\n",
    "    \n",
    "    # Add extra sentences from NLTK method if longer than others\n",
    "    sentence_lengths += [\n",
    "        (0, 0, len(nltk), 0) \n",
    "        for nltk in row['NLTK_Sentences'][len(row['Regex_Sentences']):]\n",
    "    ]\n",
    "    \n",
    "    # Add extra sentences from Transformer method if longer than others\n",
    "    sentence_lengths += [\n",
    "        (0, 0, 0, len(transformer)) \n",
    "        for transformer in row['Transformer_Sentences'][len(row['Regex_Sentences']):]\n",
    "    ]\n",
    "    \n",
    "    return sentence_lengths\n",
    "\n",
    "# Apply this function to the DataFrame to calculate mismatches\n",
    "df['Sentence_Lengths_Mismatch'] = df.apply(you_a_mismatch, axis=1)\n",
    "\n",
    "# Display the resulting mismatches column\n",
    "print(df[['Text', 'Sentence_Lengths_Mismatch']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mismatches(row):\n",
    "    # Count mismatches by comparing sentences detected by each method\n",
    "    mismatch_count = sum(1 for m, s in zip(row['Regex_Sentences'], row['Spacy_Sentences']) if m != s)\n",
    "    # Add mismatches for any extra sentences in either method\n",
    "    mismatch_count += abs(len(row['Regex_Sentences']) - len(row['Spacy_Sentences']))\n",
    "    return mismatch_count\n",
    "\n",
    "# Apply the function to create a column with the total number of mismatches\n",
    "df['Total_Mismatches'] = df.apply(count_mismatches, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sentence_peek1_1112.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing performance\n",
    "\n",
    "fdf = df[df['Total_Mismatches'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dda3671ff3204f7c08d6fd700d73245b2febe83cbc27407a5dbd57f565cde480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

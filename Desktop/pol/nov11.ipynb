{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emzou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "da= pd.read_csv(\"nov11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "da['Tokens_E'] = da['Tokens_E'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dei_bigram (tokens): \n",
    "    for i in range (len(tokens) -1): \n",
    "        if tokens[i] == \"dei\": \n",
    "            return f\"dei {tokens[i+1]}\"\n",
    "    return np.nan\n",
    "\n",
    "da['Bigram']= da['Tokens_E'].apply(get_dei_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = da[['Bigram', 'Tokens_E']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11     926\n",
       "9      892\n",
       "14     871\n",
       "12     854\n",
       "8      847\n",
       "      ... \n",
       "106      1\n",
       "181      1\n",
       "221      1\n",
       "143      1\n",
       "177      1\n",
       "Name: Len, Length: 168, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Len'] = df['Tokens_E'].apply(len)\n",
    "df['Len'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this needs to be fixed later:: some sentences were improperly split / need to account for weird text/punctuation. for now, going to limit data to sentences < 50 tokens.\n",
    "\n",
    "sentlength = list(df['Len'].value_counts())\n",
    "from statistics import mean\n",
    "from statistics import median\n",
    "\n",
    "mean(sentlength)\n",
    "median(sentlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Len'] <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emzou\\AppData\\Local\\Temp\\ipykernel_10908\\2971193950.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Contextual_Embedding'] = df['Tokens_E'].apply(get_embedding)\n",
      "C:\\Users\\emzou\\AppData\\Local\\Temp\\ipykernel_10908\\2971193950.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Word2'] = df['Bigram'].apply(lambda x: x.split()[1])\n",
      "C:\\Users\\emzou\\AppData\\Local\\Temp\\ipykernel_10908\\2971193950.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Contextual_Embedding']= df['Tokens_E'].apply(get_embedding)\n",
      "C:\\Users\\emzou\\AppData\\Local\\Temp\\ipykernel_10908\\2971193950.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Embedding_Dim'] = df['Contextual_Embedding'].apply(lambda x: x.size(0) if isinstance(x, torch.Tensor) else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Bigram                                           Tokens_E  Len  \\\n",
      "0        dei roles  [tumblr, girls, were, the, nerdy, outcasts, wh...   20   \n",
      "1         dei good  [just, robbing, and, dispossesing, innocent, c...   31   \n",
      "2         dei mind  [the, dei, mind, virus, has, infected, every, ...   11   \n",
      "5        dei asset  [i, ca, tell, actual, practicable, advice, fro...   25   \n",
      "6           dei is                 [dei, is, the, law, of, the, land]    7   \n",
      "...            ...                                                ...  ...   \n",
      "23432  dei minions  [milley, and, the, dei, minions, basically, se...   19   \n",
      "23433   dei hiring  [he's, in, charge, of, dei, hiring, in, the, w...    9   \n",
      "23434     dei hire                         [kamala, is, a, dei, hire]    5   \n",
      "23435      dei has  [the, can, barely, gaslight, this, within, the...   35   \n",
      "23436   dei troons  [dei, troons, wars, expensive, shit, and, raci...   12   \n",
      "\n",
      "         Word2                               Contextual_Embedding  \\\n",
      "0        roles  [[tensor(-0.3412), tensor(0.0264), tensor(-0.0...   \n",
      "1         good  [[tensor(-0.1165), tensor(-0.0469), tensor(0.0...   \n",
      "2         mind  [[tensor(-0.2025), tensor(0.1280), tensor(-0.0...   \n",
      "5        asset  [[tensor(-0.0853), tensor(0.1722), tensor(0.04...   \n",
      "6           is  [[tensor(-0.3797), tensor(-0.1023), tensor(-0....   \n",
      "...        ...                                                ...   \n",
      "23432  minions  [[tensor(-0.3307), tensor(0.1355), tensor(0.08...   \n",
      "23433   hiring  [[tensor(0.1787), tensor(0.3718), tensor(0.154...   \n",
      "23434     hire  [[tensor(-0.3167), tensor(-0.1979), tensor(-0....   \n",
      "23435      has  [[tensor(-0.2025), tensor(0.1280), tensor(-0.0...   \n",
      "23436   troons  [[tensor(-0.3797), tensor(-0.1023), tensor(-0....   \n",
      "\n",
      "       Embedding_Dim  \n",
      "0                 20  \n",
      "1                 31  \n",
      "2                 11  \n",
      "5                 25  \n",
      "6                  7  \n",
      "...              ...  \n",
      "23432             19  \n",
      "23433              9  \n",
      "23434              5  \n",
      "23435             35  \n",
      "23436             12  \n",
      "\n",
      "[21164 rows x 6 columns]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [27, 768] at entry 0 and [8, 768] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m dei_hire_embedding\u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBigram\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdei hire\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextual_Embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     22\u001b[0m dei_hire_tensors \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(emb\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(emb, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m emb \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m dei_hire_embedding]\n\u001b[1;32m---> 23\u001b[0m target_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdei_hire_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContextual_Embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: cosine_similarity(x, target_embedding)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [27, 768] at entry 0 and [8, 768] at entry 1"
     ]
    }
   ],
   "source": [
    "def get_embedding(text): \n",
    "    inputs = tokenizer(text, return_tensors = \"pt\", truncation = True, padding = True)\n",
    "    with torch.no_grad(): \n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  \n",
    "    return cls_embedding\n",
    "\n",
    "df['Contextual_Embedding'] = df['Tokens_E'].apply(get_embedding) \n",
    "\n",
    "df\n",
    "\n",
    "df['Word2'] = df['Bigram'].apply(lambda x: x.split()[1])\n",
    "\n",
    "df['Contextual_Embedding']= df['Tokens_E'].apply(get_embedding)\n",
    "\n",
    "df['Embedding_Dim'] = df['Contextual_Embedding'].apply(lambda x: x.size(0) if isinstance(x, torch.Tensor) else None)\n",
    "inconsistent_rows = df[df['Embedding_Dim'] != 768]\n",
    "print(inconsistent_rows)\n",
    "\n",
    "dei_hire_embedding= df[df['Bigram'] == \"dei hire\"]['Contextual_Embedding'].values\n",
    "\n",
    "dei_hire_tensors = [torch.tensor(emb.numpy()) if isinstance(emb, torch.Tensor) else emb for emb in dei_hire_embedding]\n",
    "target_embedding = torch.stack(dei_hire_tensors).mean(dim=0)\n",
    "\n",
    "df['Similarity'] = df['Contextual_Embedding'].apply(lambda x: cosine_similarity(x, target_embedding)[0][0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_bigram = \"dei hire\"\n",
    "\n",
    "inputs_target = tokenizer(target_bigram, return_tensors = \"pt\")\n",
    "candidate_bigrams = list(set(df['Bigram'].dropna().unique())) \n",
    "inputs_candidates = tokenizer(candidate_bigrams, padding = True, truncation = True, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (3608) does not match length of index (23437)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m     candidate_embeddings \u001b[38;5;241m=\u001b[39m model (\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs_candidates)\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      5\u001b[0m similarities \u001b[38;5;241m=\u001b[39m cosine_similarity([target_embedding], candidate_embeddings)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSimilarity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39m [similarities[\u001b[38;5;241m0\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(similarities[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[0;32m      8\u001b[0m inputs_candidates \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;28mlist\u001b[39m(candidate_bigrams), padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, truncation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4535\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (3608) does not match length of index (23437)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    target_embedding = model (**inputs_target).last_hidden_state.mean(dim = 1).squeeze().numpy()\n",
    "    candidate_embeddings = model (**inputs_candidates).last_hidden_state.mean(dim = 1).numpy()\n",
    "\n",
    "similarities = cosine_similarity([target_embedding], candidate_embeddings)\n",
    "\n",
    "df['Similarity']= [similarities[0][i] for i in range(len(similarities[0]))]\n",
    "inputs_candidates = tokenizer(list(candidate_bigrams), padding = True, truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    target_embedding = model (**inputs_target).last_hidden_state.mean(dim = 1).squeeze().numpy()\n",
    "    candidate_embeddings = model (**inputs_candidates).last_hidden_state.mean(dim = 1).numpy()\n",
    "\n",
    "similarities = cosine_similarity([target_embedding], candidate_embeddings)\n",
    "\n",
    "df['Similarity']= [similarities[0][i] for i in range(len(similarities[0]))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
